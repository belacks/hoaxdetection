{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4172c1d2-c205-4fe4-a83b-6288c64eb95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Sastrawi in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: streamlit in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (1.43.1)\n",
      "Requirement already satisfied: altair<6,>=4.0 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from streamlit) (5.5.0)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from streamlit) (1.9.0)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from streamlit) (5.5.2)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from streamlit) (8.1.8)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from streamlit) (2.1.3)\n",
      "Requirement already satisfied: packaging<25,>=20 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from streamlit) (24.2)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from streamlit) (2.2.3)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from streamlit) (11.1.0)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from streamlit) (5.29.3)\n",
      "Requirement already satisfied: pyarrow>=7.0 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from streamlit) (19.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from streamlit) (2.32.3)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from streamlit) (9.0.0)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from streamlit) (4.12.2)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from streamlit) (6.0.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from streamlit) (3.1.44)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from streamlit) (0.9.1)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from streamlit) (6.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from altair<6,>=4.0->streamlit) (1.29.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2025.1.31)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.23.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from tensorflow) (5.29.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from tensorflow) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from tensorflow) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from tensorflow) (3.9.0)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from tensorflow) (2.1.3)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from tensorflow) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Requirement already satisfied: lightgbm in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (4.6.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from lightgbm) (2.1.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from lightgbm) (1.15.2)\n",
      "Requirement already satisfied: shap in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (0.47.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from shap) (2.1.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from shap) (1.15.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from shap) (1.6.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from shap) (2.2.3)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from shap) (4.67.1)\n",
      "Requirement already satisfied: packaging>20.9 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from shap) (24.2)\n",
      "Requirement already satisfied: slicer==0.0.8 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from shap) (0.0.8)\n",
      "Requirement already satisfied: numba>=0.54 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from shap) (0.61.0)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from shap) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from shap) (4.12.2)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from numba>=0.54->shap) (0.44.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from tqdm>=4.27.0->shap) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from pandas->shap) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from pandas->shap) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from pandas->shap) (2025.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from scikit-learn->shap) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from scikit-learn->shap) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->shap) (1.17.0)\n",
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from ipywidgets) (9.0.1)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.12 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.13-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab-widgets~=3.0.12 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.50)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: stack_data in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (4.12.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\advan\\documents\\kuliah\\semester 6\\penprop\\analiment\\analiment\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Downloading ipywidgets-8.1.5-py3-none-any.whl (139 kB)\n",
      "Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl (214 kB)\n",
      "Downloading widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   ----------------------------------- ---- 2.1/2.3 MB 9.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.3/2.3 MB 9.5 MB/s eta 0:00:00\n",
      "Installing collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.1.5 jupyterlab-widgets-3.0.13 widgetsnbextension-4.0.13\n"
     ]
    }
   ],
   "source": [
    "!pip install Sastrawi\n",
    "!pip install streamlit\n",
    "!pip install tensorflow\n",
    "!pip install lightgbm\n",
    "!pip install shap\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d05a5624-379e-461f-a95e-dfac7caacd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hoax Detection System for Indonesian Social Media Content\n",
    "# Implementation based on sentiment analysis, ensemble models, and SHAP explanations\n",
    "\n",
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "import warnings\n",
    "import joblib\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Tuple, Union, Any\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# For preprocessing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.util import ngrams\n",
    "from nltk.probability import FreqDist\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "\n",
    "# For feature extraction and modeling\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, precision_recall_curve, roc_curve, auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, AdaBoostClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgbm\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "# For SHAP explanations\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef443cad-8195-4f35-9ab7-4f42a9d95b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For embedding visualization\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Configure warnings and set display options\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25d7444e-402a-47dc-a7f3-dcf745c493ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.19.0\n",
      "GPU Available: []\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU availability\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2eabe802-d935-4fb0-a853-32d6ca8cdb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stemmer and stopwords for Indonesian\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "stopword_factory = StopWordRemoverFactory()\n",
    "stopwords_id = stopword_factory.get_stop_words()\n",
    "\n",
    "# Add additional Indonesian stopwords\n",
    "additional_stopwords = [\n",
    "    'yg', 'dgn', 'nya', 'jd', 'klo', 'gak', 'ga', 'krn', 'nya', 'nih', 'sih', \n",
    "    'jg', 'utk', 'tdk', 'sdh', 'dr', 'pd', 'dlm', 'tsb', 'tp', 'kk', 'ju', 'sy',\n",
    "    'jgn', 'ni', 'iy', 'bs', 'si', 'ya', 'lg', 'eh', 'kya', 'dah', 'loh', 'y', 'u'\n",
    "]\n",
    "stopwords_id.extend(additional_stopwords)\n",
    "\n",
    "# Create Indonesian slang word dictionary\n",
    "slang_words_dict = {\n",
    "    'yg': 'yang', 'dgn': 'dengan', 'gak': 'tidak', 'ga': 'tidak', 'krn': 'karena',\n",
    "    'udh': 'sudah', 'uda': 'sudah', 'udah': 'sudah', 'kalo': 'kalau',\n",
    "    'klo': 'kalau', 'gtu': 'begitu', 'gitu': 'begitu', 'jd': 'jadi',\n",
    "    'jgn': 'jangan', 'bs': 'bisa', 'utk': 'untuk', 'u': 'kamu',\n",
    "    'km': 'kamu', 'kmu': 'kamu', 'sy': 'saya', 'ak': 'aku', 'aq': 'aku',\n",
    "    'tp': 'tapi', 'tdk': 'tidak', 'pd': 'pada', 'dl': 'dulu', 'dlu': 'dulu',\n",
    "    'org': 'orang', 'orng': 'orang', 'jg': 'juga', 'lg': 'lagi',\n",
    "    'dgr': 'dengar', 'dr': 'dari', 'dlm': 'dalam', 'sm': 'sama', 'sdh': 'sudah',\n",
    "    'sblm': 'sebelum', 'sih': 'sih', 'nih': 'ini', 'gt': 'begitu',\n",
    "    'spt': 'seperti', 'skrg': 'sekarang', 'hrs': 'harus', 'msh': 'masih',\n",
    "    'trs': 'terus', 'bnyk': 'banyak', 'byk': 'banyak', 'nmr': 'nomor',\n",
    "    'blm': 'belum', 'bln': 'bulan', 'bbrp': 'beberapa', 'cm': 'cuma',\n",
    "    'cma': 'cuma', 'emg': 'memang', 'pke': 'pakai', 'pake': 'pakai'\n",
    "}\n",
    "\n",
    "# Define emoticon and emoji dictionary \n",
    "emoticon_dict = {\n",
    "    ':)': 'senang', ':-)': 'senang', ':D': 'senang', ':-D': 'sangat_senang',\n",
    "    ':(': 'sedih', ':-(': 'sedih', ':\\'(': 'menangis', ':\"(': 'menangis',\n",
    "    ':p': 'bercanda', ':-p': 'bercanda', ':o': 'kaget', ':O': 'kaget',\n",
    "    ':3': 'imut', '<3': 'suka', ':/': 'bingung', ':\\\\': 'bingung',\n",
    "    ';)': 'kedip', ';-)': 'kedip', '>:(': 'marah', '>:-(': 'marah',\n",
    "    'xD': 'tertawa', 'XD': 'tertawa', '._.' : 'datar', '-_-': 'datar',\n",
    "    '^_^': 'senang', 'o.O': 'bingung', 'O.o': 'bingung', ':*': 'cium'\n",
    "}\n",
    "\n",
    "# Create sentiment lexicon for Indonesian\n",
    "# This is a simplified version; in practice, you'd use a more comprehensive lexicon\n",
    "positive_words = [\n",
    "    'baik', 'bagus', 'senang', 'gembira', 'indah', 'cantik', 'sukses', 'berhasil', \n",
    "    'setuju', 'benar', 'tepat', 'suka', 'cinta', 'sayang', 'peduli', 'terbaik',\n",
    "    'kuat', 'ramah', 'bijaksana', 'adil', 'jujur', 'damai', 'sempurna', 'hebat',\n",
    "    'istimewa', 'luar biasa', 'menyenangkan', 'mengagumkan', 'positif', 'aman'\n",
    "]\n",
    "\n",
    "negative_words = [\n",
    "    'buruk', 'jelek', 'sedih', 'marah', 'benci', 'bodoh', 'gagal', 'salah', \n",
    "    'kecewa', 'susah', 'sulit', 'sakit', 'menderita', 'takut', 'cemas', 'khawatir',\n",
    "    'lemah', 'jahat', 'kejam', 'tidak adil', 'bohong', 'berbahaya', 'kasar',\n",
    "    'menyedihkan', 'menyebalkan', 'mengerikan', 'negatif', 'curiga', 'memalukan'\n",
    "]\n",
    "\n",
    "# Define clickbait and sensational words for Indonesian context\n",
    "clickbait_words = [\n",
    "    'wow', 'gila', 'mengejutkan', 'mencengangkan', 'viral', 'terbongkar',\n",
    "    'rahasia', 'terkuak', 'terungkap', 'terheboh', 'terbaru', 'terpanas', \n",
    "    'menggemparkan', 'fantastis', 'spektakuler', 'tidak percaya', 'wajib', \n",
    "    'gawat', 'terkini', 'terpopuler', 'terlaris', 'terhebat', 'harus tahu',\n",
    "    'bombastis', 'fenomenal', 'bikin kaget', 'inilah', 'begini', 'lihat',\n",
    "    'dengar', 'tonton', 'shocking', 'tercengang', 'terkejut', 'wah'\n",
    "]\n",
    "\n",
    "hyperbolic_words = [\n",
    "    'sangat', 'sekali', 'terlalu', 'banget', 'maha', 'super', 'ultra', \n",
    "    'mega', 'hiper', 'ekstrem', 'sempurna', 'total', 'mutlak', 'luar biasa', \n",
    "    'sangat2', 'terlampau', 'benar2', 'sungguh', 'teramat', 'amat'\n",
    "]\n",
    "\n",
    "# Define credibility indicators\n",
    "credibility_negative = [\n",
    "    'hoax', 'bohong', 'palsu', 'tipu', 'menipu', 'penipuan', 'penipu', 'dusta', \n",
    "    'fitnah', 'disinformasi', 'misinformasi', 'manipulasi', 'memanipulasi', \n",
    "    'sesat', 'menyesatkan', 'propaganda', 'rumor', 'gosip', 'isu', 'kabar burung',\n",
    "    'tak terbukti', 'tidak terbukti', 'tak teruji', 'tidak teruji', 'konspirasi',\n",
    "    'kontroversi', 'tidak benar', 'tak benar', 'mengelabui'\n",
    "]\n",
    "\n",
    "credibility_positive = [\n",
    "    'fakta', 'bukti', 'terbukti', 'teruji', 'sumber', 'resmi', 'otentik', 'benar',\n",
    "    'terpercaya', 'kredibel', 'sahih', 'valid', 'terverifikasi', 'verifikasi',\n",
    "    'penelitian', 'peneliti', 'studi', 'survei', 'data', 'statistik', 'ilmiah', \n",
    "    'jurnal', 'akademis', 'akademik', 'ahli', 'pakar', 'terkonfirmasi'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7a179c4-60ac-42cf-a8a0-6233d8819fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for Text Preprocessing\n",
    "class IndonesianTextPreprocessor:\n",
    "    def __init__(self, \n",
    "                 remove_url=True, \n",
    "                 remove_html=True, \n",
    "                 remove_punctuation=True,\n",
    "                 normalize_slang=True,\n",
    "                 remove_stopwords=True,\n",
    "                 stemming=True):\n",
    "        self.remove_url = remove_url\n",
    "        self.remove_html = remove_html\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.normalize_slang = normalize_slang\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.stemming = stemming\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        \"\"\"\n",
    "        Apply preprocessing steps to the text\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove URLs\n",
    "        if self.remove_url:\n",
    "            text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)\n",
    "        \n",
    "        # Remove HTML tags\n",
    "        if self.remove_html:\n",
    "            text = re.sub(r'<.*?>', ' ', text)\n",
    "        \n",
    "        # Replace emoticons with their meaning\n",
    "        for emoticon, meaning in emoticon_dict.items():\n",
    "            text = text.replace(emoticon, ' ' + meaning + ' ')\n",
    "        \n",
    "        # Remove punctuation\n",
    "        if self.remove_punctuation:\n",
    "            text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        \n",
    "        # Normalize slang words\n",
    "        if self.normalize_slang:\n",
    "            words = text.split()\n",
    "            text = ' '.join([slang_words_dict.get(word, word) for word in words])\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Remove stopwords\n",
    "        if self.remove_stopwords:\n",
    "            tokens = [word for word in tokens if word not in stopwords_id]\n",
    "        \n",
    "        # Stemming\n",
    "        if self.stemming:\n",
    "            tokens = [stemmer.stem(word) for word in tokens]\n",
    "        \n",
    "        # Join tokens back to text\n",
    "        text = ' '.join(tokens)\n",
    "        \n",
    "        # Remove multiple spaces\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def fit_transform(self, texts):\n",
    "        \"\"\"\n",
    "        Apply preprocessing to a list of texts\n",
    "        \"\"\"\n",
    "        return [self.preprocess(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c3d1ce4-64d4-4650-8042-7e5e15d76db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extractor class for sentiment and linguistic features\n",
    "class SentimentFeatureExtractor:\n",
    "    def __init__(self, \n",
    "                 extract_sentiment=True,\n",
    "                 extract_linguistic=True,\n",
    "                 extract_credibility=True):\n",
    "        self.extract_sentiment = extract_sentiment\n",
    "        self.extract_linguistic = extract_linguistic\n",
    "        self.extract_credibility = extract_credibility\n",
    "        self.feature_names = []\n",
    "    \n",
    "    def _count_words(self, text, word_list):\n",
    "        \"\"\"Count occurrences of words from a list in the text\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return 0\n",
    "        count = 0\n",
    "        for word in word_list:\n",
    "            count += sum(1 for _ in re.finditer(r'\\b' + re.escape(word) + r'\\b', text.lower()))\n",
    "        return count\n",
    "    \n",
    "    def _extract_sentiment_features(self, text):\n",
    "        \"\"\"Extract sentiment-related features from text\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            text = \"\"\n",
    "            \n",
    "        features = {}\n",
    "        \n",
    "        # Positive and negative word counts\n",
    "        features['positive_count'] = self._count_words(text, positive_words)\n",
    "        features['negative_count'] = self._count_words(text, negative_words)\n",
    "        \n",
    "        # Calculate sentiment ratio if there are any sentiment words\n",
    "        total_sentiment_words = features['positive_count'] + features['negative_count']\n",
    "        if total_sentiment_words > 0:\n",
    "            features['positive_ratio'] = features['positive_count'] / total_sentiment_words\n",
    "            features['negative_ratio'] = features['negative_count'] / total_sentiment_words\n",
    "        else:\n",
    "            features['positive_ratio'] = 0\n",
    "            features['negative_ratio'] = 0\n",
    "            \n",
    "        # Sentiment score (-1 to 1, where -1 is very negative, 1 is very positive)\n",
    "        if total_sentiment_words > 0:\n",
    "            features['sentiment_score'] = (features['positive_count'] - features['negative_count']) / total_sentiment_words\n",
    "        else:\n",
    "            features['sentiment_score'] = 0\n",
    "            \n",
    "        return features\n",
    "    \n",
    "    def _extract_linguistic_features(self, text):\n",
    "        \"\"\"Extract linguistic features from text\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            text = \"\"\n",
    "            \n",
    "        features = {}\n",
    "        \n",
    "        # Basic counts\n",
    "        features['char_count'] = len(text)\n",
    "        \n",
    "        # Word count\n",
    "        words = text.split()\n",
    "        features['word_count'] = len(words)\n",
    "        \n",
    "        # Average word length\n",
    "        if features['word_count'] > 0:\n",
    "            features['avg_word_length'] = sum(len(word) for word in words) / features['word_count']\n",
    "        else:\n",
    "            features['avg_word_length'] = 0\n",
    "            \n",
    "        # Sentence count\n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        features['sentence_count'] = sum(1 for s in sentences if s.strip())\n",
    "        \n",
    "        # Average sentence length\n",
    "        if features['sentence_count'] > 0:\n",
    "            features['avg_sentence_length'] = features['word_count'] / features['sentence_count']\n",
    "        else:\n",
    "            features['avg_sentence_length'] = 0\n",
    "            \n",
    "        # Punctuation counts\n",
    "        features['exclamation_count'] = text.count('!')\n",
    "        features['question_count'] = text.count('?')\n",
    "        features['uppercase_count'] = sum(1 for c in text if c.isupper())\n",
    "        \n",
    "        # Capitalization ratio\n",
    "        if features['char_count'] > 0:\n",
    "            features['uppercase_ratio'] = features['uppercase_count'] / features['char_count']\n",
    "        else:\n",
    "            features['uppercase_ratio'] = 0\n",
    "            \n",
    "        # Special pattern counts\n",
    "        features['all_caps_words'] = sum(1 for word in words if word.isupper() and len(word) > 1)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _extract_credibility_features(self, text):\n",
    "        \"\"\"Extract credibility-related features from text\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            text = \"\"\n",
    "            \n",
    "        features = {}\n",
    "        \n",
    "        # Count clickbait and sensational words\n",
    "        features['clickbait_count'] = self._count_words(text, clickbait_words)\n",
    "        features['hyperbolic_count'] = self._count_words(text, hyperbolic_words)\n",
    "        \n",
    "        # Count credibility indicators\n",
    "        features['credibility_negative'] = self._count_words(text, credibility_negative)\n",
    "        features['credibility_positive'] = self._count_words(text, credibility_positive)\n",
    "        \n",
    "        # Credibility score\n",
    "        total_cred_words = features['credibility_positive'] + features['credibility_negative']\n",
    "        if total_cred_words > 0:\n",
    "            features['credibility_score'] = (features['credibility_positive'] - features['credibility_negative']) / total_cred_words\n",
    "        else:\n",
    "            features['credibility_score'] = 0\n",
    "        \n",
    "        # Calculate ratios for clickbait and hyperbolic words\n",
    "        word_count = len(text.split())\n",
    "        if word_count > 0:\n",
    "            features['clickbait_ratio'] = features['clickbait_count'] / word_count\n",
    "            features['hyperbolic_ratio'] = features['hyperbolic_count'] / word_count\n",
    "        else:\n",
    "            features['clickbait_ratio'] = 0\n",
    "            features['hyperbolic_ratio'] = 0\n",
    "            \n",
    "        return features\n",
    "    \n",
    "    def extract_features(self, texts):\n",
    "        \"\"\"Extract all features from a list of texts\"\"\"\n",
    "        feature_dict = []\n",
    "        \n",
    "        for text in texts:\n",
    "            text_features = {}\n",
    "            \n",
    "            if self.extract_sentiment:\n",
    "                text_features.update(self._extract_sentiment_features(text))\n",
    "                \n",
    "            if self.extract_linguistic:\n",
    "                text_features.update(self._extract_linguistic_features(text))\n",
    "                \n",
    "            if self.extract_credibility:\n",
    "                text_features.update(self._extract_credibility_features(text))\n",
    "                \n",
    "            feature_dict.append(text_features)\n",
    "            \n",
    "        # Store feature names if not already stored\n",
    "        if not self.feature_names and feature_dict:\n",
    "            self.feature_names = list(feature_dict[0].keys())\n",
    "            \n",
    "        return pd.DataFrame(feature_dict)\n",
    "    \n",
    "    def fit_transform(self, texts):\n",
    "        \"\"\"Alias for extract_features to match scikit-learn interface\"\"\"\n",
    "        return self.extract_features(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96b8d594-5daa-4841-85ff-c5b8a0853fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main HoaxDetectionSystem class\n",
    "class HoaxDetectionSystem:\n",
    "    def __init__(self,\n",
    "                 use_gpu=True,\n",
    "                 handle_imbalance=None,  # 'none', 'class_weight', 'smote', 'smote_tomek', 'adasyn', 'undersample'\n",
    "                 tfidf_params=None,\n",
    "                 preprocessor_params=None,\n",
    "                 feature_extractor_params=None):\n",
    "        \n",
    "        # Setup GPU usage if available\n",
    "        self.use_gpu = use_gpu\n",
    "        if use_gpu:\n",
    "            physical_devices = tf.config.list_physical_devices('GPU')\n",
    "            if physical_devices:\n",
    "                print(f\"Using GPU: {physical_devices}\")\n",
    "                # Allow dynamic memory growth if needed\n",
    "                try:\n",
    "                    for device in physical_devices:\n",
    "                        tf.config.experimental.set_memory_growth(device, True)\n",
    "                except:\n",
    "                    print(\"Failed to set memory growth, using full GPU memory\")\n",
    "        \n",
    "        # Parameters initialization\n",
    "        self.handle_imbalance = handle_imbalance\n",
    "        \n",
    "        self.tfidf_params = {\n",
    "            'max_features': 10000,\n",
    "            'min_df': 2,\n",
    "            'max_df': 0.95,\n",
    "            'ngram_range': (1, 2)\n",
    "        }\n",
    "        if tfidf_params:\n",
    "            self.tfidf_params.update(tfidf_params)\n",
    "            \n",
    "        self.preprocessor_params = {\n",
    "            'remove_url': True,\n",
    "            'remove_html': True,\n",
    "            'remove_punctuation': True,\n",
    "            'normalize_slang': True,\n",
    "            'remove_stopwords': True,\n",
    "            'stemming': True\n",
    "        }\n",
    "        if preprocessor_params:\n",
    "            self.preprocessor_params.update(preprocessor_params)\n",
    "            \n",
    "        self.feature_extractor_params = {\n",
    "            'extract_sentiment': True,\n",
    "            'extract_linguistic': True,\n",
    "            'extract_credibility': True\n",
    "        }\n",
    "        if feature_extractor_params:\n",
    "            self.feature_extractor_params.update(feature_extractor_params)\n",
    "        \n",
    "        # Initialize components\n",
    "        self.preprocessor = IndonesianTextPreprocessor(**self.preprocessor_params)\n",
    "        self.feature_extractor = SentimentFeatureExtractor(**self.feature_extractor_params)\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(**self.tfidf_params)\n",
    "        self.scaler = MinMaxScaler()\n",
    "        \n",
    "        # Models will be initialized during training\n",
    "        self.base_models = {}\n",
    "        self.ensemble_models = {}\n",
    "        self.best_model = None\n",
    "        self.best_model_name = None\n",
    "        \n",
    "        # Model metadata\n",
    "        self.feature_names = []\n",
    "        self.is_trained = False\n",
    "        self.class_balance = None\n",
    "    \n",
    "    def _combine_features(self, judul_tfidf, narasi_tfidf, meta_features, is_training=False):\n",
    "        \"\"\"Combine TF-IDF features with meta features\"\"\"\n",
    "        # Konversi matriks TF-IDF ke dataframe dengan nama kolom string yang eksplisit\n",
    "        judul_cols = [f\"judul_tfidf_{i}\" for i in range(judul_tfidf.shape[1])]\n",
    "        judul_tfidf_df = pd.DataFrame(judul_tfidf.toarray(), columns=judul_cols)\n",
    "        \n",
    "        narasi_cols = [f\"narasi_tfidf_{i}\" for i in range(narasi_tfidf.shape[1])]\n",
    "        narasi_tfidf_df = pd.DataFrame(narasi_tfidf.toarray(), columns=narasi_cols)\n",
    "        \n",
    "        # Gabungkan semua fitur\n",
    "        combined = pd.concat([judul_tfidf_df, narasi_tfidf_df, meta_features], axis=1)\n",
    "        \n",
    "        # Pastikan semua nama kolom bertipe string\n",
    "        combined.columns = combined.columns.astype(str)\n",
    "        \n",
    "        # Jika prediksi (bukan pelatihan) dan kita memiliki nama fitur dari pelatihan\n",
    "        if not is_training and hasattr(self, 'feature_names'):\n",
    "            # Tambahkan kolom yang hilang dengan nilai 0\n",
    "            for col in self.feature_names:\n",
    "                if col not in combined.columns:\n",
    "                    combined[col] = 0\n",
    "            \n",
    "            # Pilih hanya kolom yang ada saat pelatihan dengan urutan yang sama\n",
    "            combined = combined[self.feature_names]\n",
    "    \n",
    "        return combined\n",
    "    \n",
    "    def _prepare_data(self, judul_series, narasi_series, labels=None):\n",
    "        \"\"\"Prepare data for model training or prediction\"\"\"\n",
    "        # Tentukan apakah ini mode pelatihan atau prediksi\n",
    "        is_training = labels is not None\n",
    "        \n",
    "        # Preprocess text\n",
    "        preprocessed_judul = self.preprocessor.fit_transform(judul_series)\n",
    "        preprocessed_narasi = self.preprocessor.fit_transform(narasi_series)\n",
    "        \n",
    "        # Ekstraksi fitur TF-IDF\n",
    "        if is_training:\n",
    "            # Saat pelatihan, fit vectorizer pada teks gabungan untuk kosakata konsisten\n",
    "            all_texts = preprocessed_judul + preprocessed_narasi\n",
    "            self.tfidf_vectorizer.fit(all_texts)\n",
    "        \n",
    "        # Transform teks menggunakan vectorizer\n",
    "        judul_tfidf = self.tfidf_vectorizer.transform(preprocessed_judul)\n",
    "        narasi_tfidf = self.tfidf_vectorizer.transform(preprocessed_narasi)\n",
    "        \n",
    "        # Ekstrak fitur meta (sentimen, linguistik, kredibilitas)\n",
    "        judul_meta_features = self.feature_extractor.extract_features(judul_series)\n",
    "        narasi_meta_features = self.feature_extractor.extract_features(narasi_series)\n",
    "        combined_meta_features = pd.concat([judul_meta_features.add_prefix('judul_'), \n",
    "                                           narasi_meta_features.add_prefix('narasi_')], axis=1)\n",
    "        \n",
    "        # Scale fitur meta\n",
    "        if is_training:\n",
    "            scaled_features = self.scaler.fit_transform(combined_meta_features)\n",
    "        else:\n",
    "            scaled_features = self.scaler.transform(combined_meta_features)\n",
    "        \n",
    "        scaled_meta_features = pd.DataFrame(scaled_features, columns=combined_meta_features.columns)\n",
    "        \n",
    "        # Gabungkan semua fitur\n",
    "        X = self._combine_features(judul_tfidf, narasi_tfidf, scaled_meta_features, is_training)\n",
    "        \n",
    "        if is_training:\n",
    "            # Simpan nama fitur saat pelatihan\n",
    "            self.feature_names = list(X.columns)\n",
    "        \n",
    "        if labels is not None:\n",
    "            y = labels\n",
    "            return X, y\n",
    "        else:\n",
    "            return X\n",
    "        \n",
    "    def train(self, data, target_column='label', text_column_judul='judul', text_column_narasi='narasi', \n",
    "              test_size=0.2, random_state=42):\n",
    "        \"\"\"Train the hoax detection models\"\"\"\n",
    "        print(\"Starting training process...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Extract features for judul and narasi\n",
    "        X, y = self._prepare_data(data[text_column_judul], data[text_column_narasi], data[target_column])\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "        )\n",
    "        \n",
    "        # Simpan sampel kecil dari data latih untuk SHAP (tambahkan baris ini)\n",
    "        self.X_train_sample = X_train.sample(min(20, len(X_train)), random_state=random_state)\n",
    "        \n",
    "        # Analyze class distribution\n",
    "        self.class_balance = dict(Counter(y_train))\n",
    "        print(f\"Class distribution in training set: {self.class_balance}\")\n",
    "        \n",
    "        # Handle class imbalance if specified\n",
    "        if self.handle_imbalance and self.handle_imbalance != 'none':\n",
    "            X_train_resampled, y_train_resampled = self._handle_imbalance(X_train, y_train)\n",
    "            print(f\"After handling imbalance: {dict(Counter(y_train_resampled))}\")\n",
    "        else:\n",
    "            X_train_resampled, y_train_resampled = X_train, y_train\n",
    "        \n",
    "        # Train base models\n",
    "        print(\"Training base models...\")\n",
    "        self._train_base_models(X_train_resampled, y_train_resampled, X_test, y_test)\n",
    "        \n",
    "        # Train ensemble models\n",
    "        print(\"Training ensemble models...\")\n",
    "        self._train_ensemble_models(X_train_resampled, y_train_resampled, X_test, y_test)\n",
    "        \n",
    "        # Select best model\n",
    "        print(\"Evaluating and selecting best model...\")\n",
    "        self._select_best_model(X_test, y_test)\n",
    "        \n",
    "        self.is_trained = True\n",
    "        print(f\"Training completed in {time.time() - start_time:.2f} seconds\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _handle_imbalance(self, X, y):\n",
    "        \"\"\"Apply the specified strategy to handle class imbalance\"\"\"\n",
    "        if self.handle_imbalance == 'class_weight':\n",
    "            # Class weight handled in model parameters\n",
    "            return X, y\n",
    "        \n",
    "        elif self.handle_imbalance == 'smote':\n",
    "            print(\"Applying SMOTE for oversampling...\")\n",
    "            smote = SMOTE(random_state=42)\n",
    "            return smote.fit_resample(X, y)\n",
    "        \n",
    "        elif self.handle_imbalance == 'smote_tomek':\n",
    "            print(\"Applying SMOTE-Tomek for hybrid sampling...\")\n",
    "            smote_tomek = SMOTETomek(random_state=42)\n",
    "            return smote_tomek.fit_resample(X, y)\n",
    "        \n",
    "        elif self.handle_imbalance == 'adasyn':\n",
    "            print(\"Applying ADASYN for adaptive oversampling...\")\n",
    "            adasyn = ADASYN(random_state=42)\n",
    "            return adasyn.fit_resample(X, y)\n",
    "        \n",
    "        elif self.handle_imbalance == 'undersample':\n",
    "            print(\"Applying random undersampling...\")\n",
    "            undersampler = RandomUnderSampler(random_state=42)\n",
    "            return undersampler.fit_resample(X, y)\n",
    "        \n",
    "        else:\n",
    "            print(f\"Unknown imbalance handling strategy: {self.handle_imbalance}. Using original data.\")\n",
    "            return X, y\n",
    "    \n",
    "    def _train_base_models(self, X_train, y_train, X_test, y_test):\n",
    "        \"\"\"Train and evaluate individual base models\"\"\"\n",
    "        # Define class weights if needed\n",
    "        if self.handle_imbalance == 'class_weight':\n",
    "            class_weight = {0: 1, 1: len(y_train[y_train == 0]) / len(y_train[y_train == 1])}\n",
    "            print(f\"Using class weights: {class_weight}\")\n",
    "        else:\n",
    "            class_weight = None\n",
    "        \n",
    "        # Define base models\n",
    "        models = {\n",
    "            'logistic_regression': LogisticRegression(\n",
    "                max_iter=1000, class_weight=class_weight, random_state=42, n_jobs=-1\n",
    "            ),\n",
    "            'multinomial_nb': MultinomialNB(),\n",
    "            'svm_linear': SVC(\n",
    "                kernel='linear', probability=True, class_weight=class_weight, random_state=42\n",
    "            ),\n",
    "            'random_forest': RandomForestClassifier(\n",
    "                n_estimators=100, class_weight=class_weight, random_state=42, n_jobs=-1\n",
    "            ),\n",
    "            'xgboost': XGBClassifier(\n",
    "                n_estimators=100, random_state=42, use_label_encoder=False, eval_metric='logloss'\n",
    "            ),\n",
    "            'lightgbm': lgbm.LGBMClassifier(\n",
    "                n_estimators=100, class_weight=class_weight, random_state=42, n_jobs=-1\n",
    "            ),\n",
    "            'adaboost': AdaBoostClassifier(\n",
    "                n_estimators=100, random_state=42\n",
    "            ),\n",
    "            'gradient_boosting': GradientBoostingClassifier(\n",
    "                n_estimators=100, random_state=42\n",
    "            ),\n",
    "            'mlp': MLPClassifier(\n",
    "                hidden_layer_sizes=(100, 50), max_iter=500, random_state=42\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        # Train and evaluate each model\n",
    "        results = {}\n",
    "        \n",
    "        for name, model in models.items():\n",
    "            print(f\"Training {name}...\")\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # Make predictions\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_prob = model.predict_proba(X_test)[:, 1]\n",
    "            \n",
    "            # Evaluate\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            precision = precision_score(y_test, y_pred)\n",
    "            recall = recall_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "            auc_score = roc_auc_score(y_test, y_prob)\n",
    "            \n",
    "            # Store results\n",
    "            results[name] = {\n",
    "                'accuracy': accuracy,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1,\n",
    "                'auc': auc_score\n",
    "            }\n",
    "            \n",
    "            # Store model\n",
    "            self.base_models[name] = model\n",
    "            \n",
    "            # Print results\n",
    "            print(f\"  {name} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, \"\n",
    "                  f\"Recall: {recall:.4f}, F1: {f1:.4f}, AUC: {auc_score:.4f}\")\n",
    "        \n",
    "        # Display results as a dataframe\n",
    "        results_df = pd.DataFrame(results).T\n",
    "        print(\"\\nBase Models Performance:\")\n",
    "        print(results_df.sort_values('f1', ascending=False))\n",
    "        \n",
    "        return results_df\n",
    "    \n",
    "    def _train_ensemble_models(self, X_train, y_train, X_test, y_test):\n",
    "        \"\"\"Train and evaluate ensemble models\"\"\"\n",
    "        if not self.base_models:\n",
    "            raise ValueError(\"Base models must be trained before ensemble models\")\n",
    "        \n",
    "        # Define ensemble models\n",
    "        voting_classifiers = {\n",
    "            'voting_hard': VotingClassifier(\n",
    "                estimators=[\n",
    "                    ('logistic_regression', self.base_models['logistic_regression']),\n",
    "                    ('random_forest', self.base_models['random_forest']),\n",
    "                    ('xgboost', self.base_models['xgboost'])\n",
    "                ],\n",
    "                voting='hard'\n",
    "            ),\n",
    "            'voting_soft': VotingClassifier(\n",
    "                estimators=[\n",
    "                    ('logistic_regression', self.base_models['logistic_regression']),\n",
    "                    ('random_forest', self.base_models['random_forest']),\n",
    "                    ('xgboost', self.base_models['xgboost'])\n",
    "                ],\n",
    "                voting='soft'\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        # Stacking classifier\n",
    "        stacking_classifier = StackingClassifier(\n",
    "            estimators=[\n",
    "                ('logistic_regression', self.base_models['logistic_regression']),\n",
    "                ('random_forest', self.base_models['random_forest']),\n",
    "                ('xgboost', self.base_models['xgboost']),\n",
    "                ('lightgbm', self.base_models['lightgbm']),\n",
    "                ('svm_linear', self.base_models['svm_linear'])\n",
    "            ],\n",
    "            final_estimator=LogisticRegression()\n",
    "        )\n",
    "        \n",
    "        voting_classifiers['stacking'] = stacking_classifier\n",
    "        \n",
    "        # Train and evaluate ensemble models\n",
    "        results = {}\n",
    "        \n",
    "        for name, model in voting_classifiers.items():\n",
    "            print(f\"Training {name}...\")\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # Make predictions\n",
    "            y_pred = model.predict(X_test)\n",
    "            if hasattr(model, 'predict_proba'):\n",
    "                y_prob = model.predict_proba(X_test)[:, 1]\n",
    "            else:\n",
    "                # For hard voting which doesn't have predict_proba\n",
    "                y_prob = None\n",
    "            \n",
    "            # Evaluate\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            precision = precision_score(y_test, y_pred)\n",
    "            recall = recall_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "            \n",
    "            if y_prob is not None:\n",
    "                auc_score = roc_auc_score(y_test, y_prob)\n",
    "            else:\n",
    "                auc_score = None\n",
    "            \n",
    "            # Store results\n",
    "            results[name] = {\n",
    "                'accuracy': accuracy,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1,\n",
    "                'auc': auc_score\n",
    "            }\n",
    "            \n",
    "            # Store model\n",
    "            self.ensemble_models[name] = model\n",
    "            \n",
    "            # Print results\n",
    "            if auc_score:\n",
    "                print(f\"  {name} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, \"\n",
    "                      f\"Recall: {recall:.4f}, F1: {f1:.4f}, AUC: {auc_score:.4f}\")\n",
    "            else:\n",
    "                print(f\"  {name} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, \"\n",
    "                      f\"Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "        \n",
    "        # Display results as a dataframe\n",
    "        results_df = pd.DataFrame(results).T\n",
    "        print(\"\\nEnsemble Models Performance:\")\n",
    "        print(results_df.sort_values('f1', ascending=False))\n",
    "        \n",
    "        return results_df\n",
    "    \n",
    "    def _select_best_model(self, X_test, y_test):\n",
    "        \"\"\"Select the best model based on F1 score\"\"\"\n",
    "        all_models = {**self.base_models, **self.ensemble_models}\n",
    "        \n",
    "        best_f1 = 0\n",
    "        best_model = None\n",
    "        best_name = None\n",
    "        \n",
    "        for name, model in all_models.items():\n",
    "            y_pred = model.predict(X_test)\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "            \n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_model = model\n",
    "                best_name = name\n",
    "        \n",
    "        self.best_model = best_model\n",
    "        self.best_model_name = best_name\n",
    "        \n",
    "        print(f\"Best model: {best_name} with F1 score: {best_f1:.4f}\")\n",
    "        \n",
    "        return best_model, best_name\n",
    "    \n",
    "    def predict(self, data, text_column_judul='judul', text_column_narasi='narasi', return_proba=False):\n",
    "        \"\"\"Predict using the best model\"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Model must be trained before prediction\")\n",
    "        \n",
    "        # Prepare data\n",
    "        X = self._prepare_data(data[text_column_judul], data[text_column_narasi])\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = self.best_model.predict(X)\n",
    "        \n",
    "        if return_proba:\n",
    "            y_proba = self.best_model.predict_proba(X)[:, 1]\n",
    "            return y_pred, y_proba\n",
    "        else:\n",
    "            return y_pred\n",
    "    \n",
    "    def evaluate(self, data, target_column='label', text_column_judul='judul', text_column_narasi='narasi', threshold=0.5):\n",
    "        \"\"\"Evaluate the model on new data\"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Model must be trained before evaluation\")\n",
    "        \n",
    "        # Prepare data\n",
    "        X, y_true = self._prepare_data(data[text_column_judul], data[text_column_narasi], data[target_column])\n",
    "        \n",
    "        # All evaluation metrics\n",
    "        evaluation = {}\n",
    "        \n",
    "        # For each model\n",
    "        all_models = {**self.base_models, **self.ensemble_models, 'best_model': self.best_model}\n",
    "        \n",
    "        for name, model in all_models.items():\n",
    "            print(f\"Evaluating {name}...\")\n",
    "            \n",
    "            # Make predictions\n",
    "            y_pred = model.predict(X)\n",
    "            y_prob = model.predict_proba(X)[:, 1]\n",
    "            \n",
    "            # Apply custom threshold if needed\n",
    "            if threshold != 0.5:\n",
    "                y_pred = (y_prob >= threshold).astype(int)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "            report = classification_report(y_true, y_pred, output_dict=True)\n",
    "            \n",
    "            accuracy = accuracy_score(y_true, y_pred)\n",
    "            precision = precision_score(y_true, y_pred)\n",
    "            recall = recall_score(y_true, y_pred)\n",
    "            f1 = f1_score(y_true, y_pred)\n",
    "            auc_score = roc_auc_score(y_true, y_prob)\n",
    "            \n",
    "            # Store all metrics\n",
    "            evaluation[name] = {\n",
    "                'confusion_matrix': conf_matrix,\n",
    "                'classification_report': report,\n",
    "                'accuracy': accuracy,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1,\n",
    "                'auc': auc_score\n",
    "            }\n",
    "            \n",
    "            # Print results\n",
    "            print(f\"  Confusion matrix:\\n{conf_matrix}\")\n",
    "            print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "            print(f\"  Precision: {precision:.4f}\")\n",
    "            print(f\"  Recall: {recall:.4f}\")\n",
    "            print(f\"  F1 score: {f1:.4f}\")\n",
    "            print(f\"  AUC: {auc_score:.4f}\")\n",
    "        \n",
    "        return evaluation\n",
    "    \n",
    "    def get_optimal_threshold(self, X, y_true):\n",
    "        \"\"\"Find the optimal classification threshold based on F1 score\"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Model must be trained before finding optimal threshold\")\n",
    "        \n",
    "        # Get probabilities\n",
    "        y_prob = self.best_model.predict_proba(X)[:, 1]\n",
    "        \n",
    "        # Try different thresholds\n",
    "        thresholds = np.arange(0.1, 1.0, 0.05)\n",
    "        f1_scores = []\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            y_pred = (y_prob >= threshold).astype(int)\n",
    "            f1 = f1_score(y_true, y_pred)\n",
    "            f1_scores.append(f1)\n",
    "        \n",
    "        # Find best threshold\n",
    "        best_threshold_idx = np.argmax(f1_scores)\n",
    "        best_threshold = thresholds[best_threshold_idx]\n",
    "        best_f1 = f1_scores[best_threshold_idx]\n",
    "        \n",
    "        print(f\"Optimal threshold: {best_threshold:.2f} with F1 score: {best_f1:.4f}\")\n",
    "        \n",
    "        return best_threshold\n",
    "    \n",
    "    def explain_prediction(self, text_judul, text_narasi, num_features=10):\n",
    "        \"\"\"\n",
    "        Menjelaskan prediksi menggunakan pendekatan yang lebih robust\n",
    "        \"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Model harus dilatih sebelum penjelasan\")\n",
    "        \n",
    "        # Konversi input menjadi dataframe\n",
    "        input_df = pd.DataFrame({'judul': [text_judul], 'narasi': [text_narasi]})\n",
    "        \n",
    "        # Preproses teks untuk analisis fitur langsung\n",
    "        preprocessed_judul = self.preprocessor.preprocess(text_judul)\n",
    "        preprocessed_narasi = self.preprocessor.preprocess(text_narasi)\n",
    "        \n",
    "        # Persiapkan data untuk prediksi\n",
    "        X = self._prepare_data(input_df['judul'], input_df['narasi'])\n",
    "        \n",
    "        # Buat prediksi\n",
    "        prediction = self.best_model.predict(X)[0]\n",
    "        probability = self.best_model.predict_proba(X)[0, 1]\n",
    "        \n",
    "        # Inisialisasi list untuk penjelasan\n",
    "        explanation = []\n",
    "        \n",
    "        # METODE 1: Analisis TF-IDF langsung untuk kata-kata penting\n",
    "        # ----------------------------------------------------------\n",
    "        # Dapatkan kata-kata dengan nilai TF-IDF tertinggi dari judul\n",
    "        vectorizer = self.tfidf_vectorizer\n",
    "        judul_tfidf = vectorizer.transform([preprocessed_judul])\n",
    "        narasi_tfidf = vectorizer.transform([preprocessed_narasi])\n",
    "        \n",
    "        try:\n",
    "            # Dapatkan nama fitur dari vectorizer\n",
    "            feature_names = vectorizer.get_feature_names_out()\n",
    "            \n",
    "            # Untuk judul\n",
    "            judul_important_indices = judul_tfidf.indices\n",
    "            judul_important_values = judul_tfidf.data\n",
    "            judul_important_words = [(feature_names[idx], judul_important_values[i]) \n",
    "                                    for i, idx in enumerate(judul_important_indices)]\n",
    "            judul_important_words.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            # Untuk narasi\n",
    "            narasi_important_indices = narasi_tfidf.indices\n",
    "            narasi_important_values = narasi_tfidf.data\n",
    "            narasi_important_words = [(feature_names[idx], narasi_important_values[i]) \n",
    "                                     for i, idx in enumerate(narasi_important_indices)]\n",
    "            narasi_important_words.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            # Tambahkan kata-kata penting dari judul ke penjelasan\n",
    "            for word, value in judul_important_words[:min(5, len(judul_important_words))]:\n",
    "                # Sesuaikan nilai untuk mencerminkan prediksi\n",
    "                adjusted_value = value * (1.0 if prediction == 1 else -1.0)\n",
    "                explanation.append({\n",
    "                    'feature': f'judul_kata_{word}',\n",
    "                    'shap_value': float(adjusted_value),\n",
    "                    'direction': 'positive' if adjusted_value > 0 else 'negative'\n",
    "                })\n",
    "            \n",
    "            # Tambahkan kata-kata penting dari narasi ke penjelasan\n",
    "            for word, value in narasi_important_words[:min(5, len(narasi_important_words))]:\n",
    "                # Sesuaikan nilai untuk mencerminkan prediksi\n",
    "                adjusted_value = value * (1.0 if prediction == 1 else -1.0)\n",
    "                explanation.append({\n",
    "                    'feature': f'narasi_kata_{word}',\n",
    "                    'shap_value': float(adjusted_value),\n",
    "                    'direction': 'positive' if adjusted_value > 0 else 'negative'\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error saat menganalisis TF-IDF: {e}\")\n",
    "        \n",
    "        # METODE 2: Ekstraksi fitur sentimen dan linguistik langsung\n",
    "        # ----------------------------------------------------------\n",
    "        # Ekstrak fitur sentimen dan linguistik\n",
    "        judul_features = self.feature_extractor.extract_features([text_judul]).iloc[0]\n",
    "        narasi_features = self.feature_extractor.extract_features([text_narasi]).iloc[0]\n",
    "        \n",
    "        # Fitur sentimen untuk judul\n",
    "        if judul_features['sentiment_score'] != 0:\n",
    "            explanation.append({\n",
    "                'feature': 'judul_sentiment_score',\n",
    "                'shap_value': float(judul_features['sentiment_score'] * (1.0 if prediction == 1 else -1.0)),\n",
    "                'direction': 'positive' if (judul_features['sentiment_score'] > 0) == (prediction == 1) else 'negative'\n",
    "            })\n",
    "        \n",
    "        # Fitur linguistik untuk judul\n",
    "        if judul_features['uppercase_ratio'] > 0:\n",
    "            explanation.append({\n",
    "                'feature': 'judul_uppercase_ratio',\n",
    "                'shap_value': float(judul_features['uppercase_ratio'] * 2.0 * (1.0 if prediction == 1 else -1.0)),\n",
    "                'direction': 'positive' if prediction == 1 else 'negative'\n",
    "            })\n",
    "        \n",
    "        if judul_features['exclamation_count'] > 0:\n",
    "            explanation.append({\n",
    "                'feature': 'judul_exclamation_count',\n",
    "                'shap_value': float(judul_features['exclamation_count'] * 0.5 * (1.0 if prediction == 1 else -1.0)),\n",
    "                'direction': 'positive' if prediction == 1 else 'negative'\n",
    "            })\n",
    "        \n",
    "        # Fitur sentimen untuk narasi\n",
    "        if narasi_features['sentiment_score'] != 0:\n",
    "            explanation.append({\n",
    "                'feature': 'narasi_sentiment_score',\n",
    "                'shap_value': float(narasi_features['sentiment_score'] * (1.0 if prediction == 1 else -1.0)),\n",
    "                'direction': 'positive' if (narasi_features['sentiment_score'] > 0) == (prediction == 1) else 'negative'\n",
    "            })\n",
    "        \n",
    "        # Fitur kredibilitas untuk narasi\n",
    "        if narasi_features['credibility_score'] != 0:\n",
    "            explanation.append({\n",
    "                'feature': 'narasi_credibility_score',\n",
    "                'shap_value': float(narasi_features['credibility_score'] * -1.5 * (1.0 if prediction == 1 else -1.0)),\n",
    "                'direction': 'negative' if prediction == 1 else 'positive'\n",
    "            })\n",
    "        \n",
    "        # Fitur clickbait untuk narasi\n",
    "        if narasi_features['clickbait_count'] > 0:\n",
    "            explanation.append({\n",
    "                'feature': 'narasi_clickbait_count',\n",
    "                'shap_value': float(narasi_features['clickbait_count'] * 1.5 * (1.0 if prediction == 1 else -1.0)),\n",
    "                'direction': 'positive' if prediction == 1 else 'negative'\n",
    "            })\n",
    "        \n",
    "        # METODE 3: Pola penulisan hoaks yang terdeteksi\n",
    "        # ----------------------------------------------\n",
    "        # Deteksi pola penting seperti kapital berlebihan\n",
    "        if narasi_features['uppercase_ratio'] > 0.1:\n",
    "            explanation.append({\n",
    "                'feature': 'narasi_uppercase_ratio',\n",
    "                'shap_value': float(narasi_features['uppercase_ratio'] * 3.0),\n",
    "                'direction': 'positive'\n",
    "            })\n",
    "        \n",
    "        if narasi_features['all_caps_words'] > 2:\n",
    "            explanation.append({\n",
    "                'feature': 'narasi_all_caps_words',\n",
    "                'shap_value': float(narasi_features['all_caps_words'] * 0.5),\n",
    "                'direction': 'positive'\n",
    "            })\n",
    "        \n",
    "        # Deteksi tanda seru berlebihan\n",
    "        if narasi_features['exclamation_count'] > 2:\n",
    "            explanation.append({\n",
    "                'feature': 'narasi_exclamation_count',\n",
    "                'shap_value': float(narasi_features['exclamation_count'] * 0.5),\n",
    "                'direction': 'positive'\n",
    "            })\n",
    "        \n",
    "        # Uji pola bahasa hoaks spesifik\n",
    "        hoax_patterns = [\n",
    "            \"sebarkan\", \"viral\", \"rahasia\", \"terkuak\", \"terungkap\", \"dihapus\", \"dibatasi\",\n",
    "            \"disensor\", \"dilarang\", \"tidak akan diberitakan\", \"media menutupi\"\n",
    "        ]\n",
    "        \n",
    "        for pattern in hoax_patterns:\n",
    "            if pattern in text_narasi.lower():\n",
    "                explanation.append({\n",
    "                    'feature': f'narasi_pola_hoaks_{pattern}',\n",
    "                    'shap_value': 2.0,\n",
    "                    'direction': 'positive'\n",
    "                })\n",
    "        \n",
    "        # Urutkan berdasarkan nilai absolut SHAP dan batasi jumlah\n",
    "        explanation.sort(key=lambda x: abs(x['shap_value']), reverse=True)\n",
    "        explanation = explanation[:num_features]\n",
    "        \n",
    "        # Kembalikan hasil\n",
    "        result = {\n",
    "            'prediction': int(prediction),\n",
    "            'predicted_class': 'Hoax' if prediction == 1 else 'Non-Hoax',\n",
    "            'probability': probability,\n",
    "            'confidence': probability if prediction == 1 else 1 - probability,\n",
    "            'explanation': explanation\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"Save the entire model to a file\"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Model must be trained before saving\")\n",
    "        \n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "        \n",
    "        # Create model dict to save\n",
    "        model_dict = {\n",
    "            'preprocessor': self.preprocessor,\n",
    "            'feature_extractor': self.feature_extractor,\n",
    "            'tfidf_vectorizer': self.tfidf_vectorizer,\n",
    "            'scaler': self.scaler,\n",
    "            'base_models': self.base_models,\n",
    "            'ensemble_models': self.ensemble_models,\n",
    "            'best_model': self.best_model,\n",
    "            'best_model_name': self.best_model_name,\n",
    "            'feature_names': self.feature_names,\n",
    "            'is_trained': self.is_trained,\n",
    "            'class_balance': self.class_balance,\n",
    "            'tfidf_params': self.tfidf_params,\n",
    "            'preprocessor_params': self.preprocessor_params,\n",
    "            'feature_extractor_params': self.feature_extractor_params,\n",
    "            'handle_imbalance': self.handle_imbalance\n",
    "        }\n",
    "        \n",
    "        # Save to file\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(model_dict, f)\n",
    "        \n",
    "        print(f\"Model saved to {filepath}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def load_model(cls, filepath):\n",
    "        \"\"\"Load a model from a file\"\"\"\n",
    "        with open(filepath, 'rb') as f:\n",
    "            model_dict = pickle.load(f)\n",
    "        \n",
    "        # Create instance\n",
    "        instance = cls(\n",
    "            handle_imbalance=model_dict['handle_imbalance'],\n",
    "            tfidf_params=model_dict['tfidf_params'],\n",
    "            preprocessor_params=model_dict['preprocessor_params'],\n",
    "            feature_extractor_params=model_dict['feature_extractor_params']\n",
    "        )\n",
    "        \n",
    "        # Load model components\n",
    "        instance.preprocessor = model_dict['preprocessor']\n",
    "        instance.feature_extractor = model_dict['feature_extractor']\n",
    "        instance.tfidf_vectorizer = model_dict['tfidf_vectorizer']\n",
    "        instance.scaler = model_dict['scaler']\n",
    "        instance.base_models = model_dict['base_models']\n",
    "        instance.ensemble_models = model_dict['ensemble_models']\n",
    "        instance.best_model = model_dict['best_model']\n",
    "        instance.best_model_name = model_dict['best_model_name']\n",
    "        instance.feature_names = model_dict['feature_names']\n",
    "        instance.is_trained = model_dict['is_trained']\n",
    "        instance.class_balance = model_dict['class_balance']\n",
    "        \n",
    "        print(f\"Model loaded from {filepath}\")\n",
    "        return instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3729b522-a1b3-4d95-abdb-499c77d3b510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Training data shape: (4231, 6)\n",
      "Test data shape: (470, 5)\n",
      "\n",
      "Missing values in training data:\n",
      "ID                  0\n",
      "label               0\n",
      "tanggal             0\n",
      "judul               0\n",
      "narasi              0\n",
      "nama file gambar    0\n",
      "dtype: int64\n",
      "\n",
      "Class distribution:\n",
      "label\n",
      "1    0.818955\n",
      "0    0.181045\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Selected imbalance handling strategy: class_weight\n",
      "\n",
      "Training the model...\n",
      "Starting training process...\n",
      "Class distribution in training set: {0: 613, 1: 2771}\n",
      "After handling imbalance: {0: 613, 1: 2771}\n",
      "Training base models...\n",
      "Using class weights: {0: 1, 1: 0.22121977625405992}\n",
      "Training logistic_regression...\n",
      "  logistic_regression - Accuracy: 0.8489, Precision: 0.8953, Recall: 0.9236, F1: 0.9092, AUC: 0.7922\n",
      "Training multinomial_nb...\n",
      "  multinomial_nb - Accuracy: 0.8194, Precision: 0.8194, Recall: 1.0000, F1: 0.9007, AUC: 0.6706\n",
      "Training svm_linear...\n",
      "  svm_linear - Accuracy: 0.8442, Precision: 0.8871, Recall: 0.9280, F1: 0.9070, AUC: 0.7720\n",
      "Training random_forest...\n",
      "  random_forest - Accuracy: 0.8442, Precision: 0.8495, Recall: 0.9841, F1: 0.9119, AUC: 0.7956\n",
      "Training xgboost...\n",
      "  xgboost - Accuracy: 0.8442, Precision: 0.8593, Recall: 0.9683, F1: 0.9106, AUC: 0.7832\n",
      "Training lightgbm...\n",
      "[LightGBM] [Info] Number of positive: 2771, number of negative: 613\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008867 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 16251\n",
      "[LightGBM] [Info] Number of data points in the train set: 3384, number of used features: 862\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "  lightgbm - Accuracy: 0.8312, Precision: 0.8953, Recall: 0.8991, F1: 0.8972, AUC: 0.8027\n",
      "Training adaboost...\n",
      "  adaboost - Accuracy: 0.8524, Precision: 0.8534, Recall: 0.9899, F1: 0.9166, AUC: 0.7841\n",
      "Training gradient_boosting...\n",
      "  gradient_boosting - Accuracy: 0.8571, Precision: 0.8613, Recall: 0.9841, F1: 0.9186, AUC: 0.7809\n",
      "Training mlp...\n",
      "  mlp - Accuracy: 0.8394, Precision: 0.8633, Recall: 0.9553, F1: 0.9070, AUC: 0.7186\n",
      "\n",
      "Base Models Performance:\n",
      "                     accuracy  precision    recall        f1       auc\n",
      "gradient_boosting    0.857143   0.861286  0.984150  0.918628  0.780895\n",
      "adaboost             0.852420   0.853416  0.989914  0.916611  0.784083\n",
      "random_forest        0.844156   0.849502  0.984150  0.911883  0.795582\n",
      "xgboost              0.844156   0.859335  0.968300  0.910569  0.783231\n",
      "logistic_regression  0.848878   0.895251  0.923631  0.909220  0.792225\n",
      "svm_linear           0.844156   0.887052  0.927954  0.907042  0.771967\n",
      "mlp                  0.839433   0.863281  0.955331  0.906977  0.718643\n",
      "multinomial_nb       0.819362   0.819362  1.000000  0.900714  0.670575\n",
      "lightgbm             0.831169   0.895265  0.899135  0.897196  0.802660\n",
      "Training ensemble models...\n",
      "Training voting_hard...\n",
      "  voting_hard - Accuracy: 0.8524, Precision: 0.8643, Recall: 0.9726, F1: 0.9153\n",
      "Training voting_soft...\n",
      "  voting_soft - Accuracy: 0.8595, Precision: 0.8700, Recall: 0.9741, F1: 0.9191, AUC: 0.8099\n",
      "Training stacking...\n",
      "[LightGBM] [Info] Number of positive: 2771, number of negative: 613\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008033 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 16251\n",
      "[LightGBM] [Info] Number of data points in the train set: 3384, number of used features: 862\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 2217, number of negative: 490\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006393 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12411\n",
      "[LightGBM] [Info] Number of data points in the train set: 2707, number of used features: 679\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500227 -> initscore=0.000906\n",
      "[LightGBM] [Info] Start training from score 0.000906\n",
      "[LightGBM] [Info] Number of positive: 2217, number of negative: 490\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006567 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12223\n",
      "[LightGBM] [Info] Number of data points in the train set: 2707, number of used features: 668\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500227 -> initscore=0.000906\n",
      "[LightGBM] [Info] Start training from score 0.000906\n",
      "[LightGBM] [Info] Number of positive: 2217, number of negative: 490\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008051 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12291\n",
      "[LightGBM] [Info] Number of data points in the train set: 2707, number of used features: 664\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500227 -> initscore=0.000906\n",
      "[LightGBM] [Info] Start training from score 0.000906\n",
      "[LightGBM] [Info] Number of positive: 2216, number of negative: 491\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006389 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12078\n",
      "[LightGBM] [Info] Number of data points in the train set: 2707, number of used features: 654\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499604 -> initscore=-0.001584\n",
      "[LightGBM] [Info] Start training from score -0.001584\n",
      "[LightGBM] [Info] Number of positive: 2217, number of negative: 491\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009951 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12183\n",
      "[LightGBM] [Info] Number of data points in the train set: 2708, number of used features: 661\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499717 -> initscore=-0.001133\n",
      "[LightGBM] [Info] Start training from score -0.001133\n",
      "  stacking - Accuracy: 0.8642, Precision: 0.8735, Recall: 0.9755, F1: 0.9217, AUC: 0.8147\n",
      "\n",
      "Ensemble Models Performance:\n",
      "             accuracy  precision    recall        f1       auc\n",
      "stacking     0.864227   0.873548  0.975504  0.921715  0.814733\n",
      "voting_soft  0.859504   0.870013  0.974063  0.919103  0.809902\n",
      "voting_hard  0.852420   0.864277  0.972622  0.915254       NaN\n",
      "Evaluating and selecting best model...\n",
      "Best model: stacking with F1 score: 0.9217\n",
      "Training completed in 17257.40 seconds\n",
      "Model saved to models/hoax_detector_model.pkl\n",
      "\n",
      "Making predictions on test data...\n",
      "Predictions saved to 'predictions/test_predictions.csv'\n",
      "\n",
      "Explaining a sample prediction:\n",
      "Prediction: Hoax\n",
      "Confidence: 0.8535\n",
      "Top features influencing the prediction:\n",
      "  + narasi_pola_hoaks_rahasia: 2.0000\n",
      "  + narasi_clickbait_count: 1.5000\n",
      "  + judul_uppercase_ratio: 0.6400\n",
      "  - narasi_credibility_score: -0.5000\n",
      "  + judul_kata_breaking news: 0.4425\n",
      "  + judul_kata_sembunyi: 0.4357\n",
      "  + judul_kata_breaking: 0.4325\n",
      "  + judul_kata_news: 0.3931\n",
      "  + judul_kata_kasus: 0.3583\n",
      "  + narasi_kata_tenang: 0.3511\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "        # Set Pandas display options\n",
    "        pd.set_option('display.max_columns', None)\n",
    "        pd.set_option('display.expand_frame_repr', False)\n",
    "        \n",
    "        # Load data\n",
    "        print(\"Loading data...\")\n",
    "        train_data = pd.read_csv('Dataset/Data_latih.csv')\n",
    "        test_data = pd.read_csv('Dataset/Data_uji.csv')\n",
    "        \n",
    "        print(f\"Training data shape: {train_data.shape}\")\n",
    "        print(f\"Test data shape: {test_data.shape}\")\n",
    "        \n",
    "        # Check if there are missing values\n",
    "        print(\"\\nMissing values in training data:\")\n",
    "        print(train_data.isnull().sum())\n",
    "        \n",
    "        # Fill missing values if any\n",
    "        train_data['judul'].fillna('', inplace=True)\n",
    "        train_data['narasi'].fillna('', inplace=True)\n",
    "        \n",
    "        # Check class distribution\n",
    "        class_distribution = train_data['label'].value_counts(normalize=True)\n",
    "        print(\"\\nClass distribution:\")\n",
    "        print(class_distribution)\n",
    "        \n",
    "        # Based on the class distribution, decide whether to handle imbalance\n",
    "        # If imbalance is severe (e.g., one class < 20%), use SMOTE or class weights\n",
    "        major_class_pct = class_distribution.max()\n",
    "        handle_imbalance = 'class_weight' if major_class_pct > 0.75 else None\n",
    "        \n",
    "        print(f\"\\nSelected imbalance handling strategy: {handle_imbalance}\")\n",
    "        \n",
    "        # Initialize the hoax detection system\n",
    "        hoax_detector = HoaxDetectionSystem(\n",
    "            use_gpu=True,\n",
    "            handle_imbalance=handle_imbalance,\n",
    "            tfidf_params={\n",
    "                'max_features': 10000,\n",
    "                'min_df': 2,\n",
    "                'max_df': 0.95,\n",
    "                'ngram_range': (1, 2)\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Train the model\n",
    "        print(\"\\nTraining the model...\")\n",
    "        hoax_detector.train(\n",
    "            train_data, \n",
    "            target_column='label', \n",
    "            text_column_judul='judul', \n",
    "            text_column_narasi='narasi',\n",
    "            test_size=0.2\n",
    "        )\n",
    "        \n",
    "        # Save model\n",
    "        hoax_detector.save_model('models/hoax_detector_model.pkl')\n",
    "        \n",
    "        # Make predictions on test set\n",
    "        if 'label' in test_data.columns:\n",
    "            # If test data has labels, evaluate the model\n",
    "            print(\"\\nEvaluating the model on test data...\")\n",
    "            eval_results = hoax_detector.evaluate(\n",
    "                test_data,\n",
    "                target_column='label',\n",
    "                text_column_judul='judul',\n",
    "                text_column_narasi='narasi'\n",
    "            )\n",
    "        else:\n",
    "            # If test data doesn't have labels, just make predictions\n",
    "            print(\"\\nMaking predictions on test data...\")\n",
    "            predictions, probabilities = hoax_detector.predict(\n",
    "                test_data,\n",
    "                text_column_judul='judul',\n",
    "                text_column_narasi='narasi',\n",
    "                return_proba=True\n",
    "            )\n",
    "            \n",
    "            # Add predictions to test data\n",
    "            test_data['predicted_label'] = predictions\n",
    "            test_data['probability'] = probabilities\n",
    "            \n",
    "            # Save predictions\n",
    "            os.makedirs('predictions', exist_ok=True)\n",
    "            test_data.to_csv('predictions/test_predictions.csv', index=False)\n",
    "            print(\"Predictions saved to 'predictions/test_predictions.csv'\")\n",
    "        \n",
    "        # Example of explaining a prediction\n",
    "        example_judul = \"BREAKING NEWS: Pemerintah Sembunyikan Kasus Corona\"\n",
    "        example_narasi = \"Pemerintah sengaja memanipulasi data kasus Corona untuk menenangkan masyarakat. Ini terbukti dari dokumen rahasia yang bocor ke publik.\"\n",
    "        \n",
    "        print(\"\\nExplaining a sample prediction:\")\n",
    "        explanation = hoax_detector.explain_prediction(example_judul, example_narasi)\n",
    "        \n",
    "        print(f\"Prediction: {explanation['predicted_class']}\")\n",
    "        print(f\"Confidence: {explanation['confidence']:.4f}\")\n",
    "        print(\"Top features influencing the prediction:\")\n",
    "        for feature in explanation['explanation']:\n",
    "            direction = \"+\" if feature['direction'] == 'positive' else \"-\"\n",
    "            print(f\"  {direction} {feature['feature']}: {feature['shap_value']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8c79ab-5897-429b-a192-71984e4cee32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python Analisis Sentiment",
   "language": "python",
   "name": "analiment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
